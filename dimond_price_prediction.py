# -*- coding: utf-8 -*-
"""Dimond_Price_Prediction.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1WCrRWeZ-xxU9jAlIG5xKU1eAJzdfNMyn

# Capstone Project 15: Diamond Price Prediction

---

### Context

Diamond is one of the precious stones which are always in huge demand in the investment market. Diamonds are also used in many industrial applications like cutting but it is mostly used as a gemstone. The actual price of a diamond however is determined by a gemologist after examining its various features such as its carat, cut, color, and clarity. Dimensions of a diamond is also a very important parameter to determine its worth. Nearly, 142 million carats of diamonds were produced worldwide in 2019 alone. This makes it very important to come up with some smart technique to estimate its worth.

---

### Problem Statement

A diamond distributor decided to put almost 2000 diamonds for auction. A jewellery company is interested in making a bid to purchase these diamonds in order to expand their business. As a data scientist, your job is to build a prediction model to predict the price of diamonds so that your company knows how much it should bid.

---

---

### Data Description

The **diamonds** dataset contains the prices and other attributes of almost 54,000 diamonds. Following are the attributes:  


|Column|Description|
|---:|:---|
|`carat`|weight of the diamond|
|`cut`|quality of the cut|
|`color`|diamond colour, from J (worst) to D (best)|
|`clarity`|a measurement of how clear the diamond is (I1 (worst), SI2, SI1, VS2, VS1, VVS2, VVS1, IF (best))|
|`table`|The width of the diamond's table expressed as a percentage of its average diameter|
|`price`|price in US dollars|
|`x`|length in mm|
|`y`|width in mm|
|`z`|depth in mm|
|`depth`|total depth percentage = $\frac{2z}{x + y}$|

  **Dataset Link:** https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/diamonds.csv

---

#### 1. Import Modules and Load Dataset

Link to the dataset: https://student-datasets-bucket.s3.ap-south-1.amazonaws.com/whitehat-ds-datasets/diamonds.csv
"""

# Import the required modules and load the dataset.
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns

# Get the information on DataFrame.
df=pd.read_csv('diamonds.csv')
df.head(100)

# Check if there are any null values. If any column has null values, treat them accordingly
df.info()

# Drop 'Unnamed: 0' column as it is of no use 
df.drop(columns='Unnamed: 0',inplace=True)

df

"""---

#### 2. Exploratory Data Analysis

We need to predict the value of `price` variable, using other variables. Thus, `price` is the target or dependent variable and other columns except `price` are the features or the independent variables.

Perform the following tasks:

- Create Box plots between each **categorical** variable and the target variable `price` to sense the distribution of values.

- Create the Scatter plots between each **numerical** variable and the target variable `price`. Determine which variable(s) shows linear relationship with the target variable `price`.

- Create a normal distribution curve for the `price`.
"""

# Boxplot for 'cut' vs 'price'
plt.figure(figsize=(5,6),dpi=96)
plt.title('Box plot for cut and price')
sns.boxplot(y='price',x='cut',data=df)
plt.show()

# Boxplot for 'color' vs 'price'
plt.figure(figsize=(5,6),dpi=96)
plt.title('Box plot for color and price')
sns.boxplot(y='price',x='color',data=df)
plt.show()

# Boxplot for 'clarity' vs 'price'
plt.figure(figsize=(5,6),dpi=96)
plt.title('Box plot for clarity and price')
sns.boxplot(y='price',x='clarity',data=df)
plt.show()

# Create scatter plot with 'carat' on X-axis and 'price' on Y-axis
plt.figure(figsize=(12,4),dpi=96)
plt.title('Scatter plot for carat and price')
plt.xlabel('Price')
plt.ylabel('Carat')
plt.scatter(x='price',y='carat',data=df)
plt.show()

# Create scatter plot with 'depth' on X-axis and 'price' on Y-axis
plt.figure(figsize=(12,4),dpi=96)
plt.title('Scatter plot for depth and price')
plt.xlabel('Price')
plt.ylabel('Depth')
plt.scatter(x='price',y='depth',data=df)
plt.show()

# Create scatter plot with 'table' on X-axis and 'price' on Y-axis
plt.figure(figsize=(12,4),dpi=96)
plt.title('Scatter plot for table and price')
plt.xlabel('Price')
plt.ylabel('Tabel')
plt.scatter(x='price',y='table',data=df)
plt.show()

# Create scatter plot with attribute 'x' on X-axis and 'price' on Y-axis
plt.figure(figsize=(12,4),dpi=96)
plt.title('Scatter plot for x and price')
plt.xlabel('Price')
plt.ylabel('X')
plt.scatter(x='price',y='x',data=df)
plt.show()

# Create scatter plot with attribute 'y' on X-axis and 'price' on Y-axis
plt.figure(figsize=(12,4),dpi=96)
plt.title('Scatter plot for y and price')
plt.xlabel('Price')
plt.ylabel('Y')
plt.scatter(x='price',y='y',data=df)
plt.show()

# Create scatter plot with 'z' on X-axis and 'price' on Y-axis
plt.figure(figsize=(12,4),dpi=96)
plt.title('Scatter plot for z and price')
plt.xlabel('Price')
plt.ylabel('Z')
plt.scatter(x='price',y='z',data=df)
plt.show()

"""**Q:** Which attribute exhibit the best linear relationship with the target variable `price`?

**A:**carat attribute exhibit the best linear relationship with the target variable.
"""

# Create a normal distribution curve for the `price`.
from scipy.stats import norm
price=df['price'].values
# Create a probablity density function for plotting the normal distribution

def prob_den_func(arr):
  constant=1/(arr.std()*np.sqrt(2*np.pi))
  e_power=-(((arr-arr.mean())**2))/(2*(arr.std())**2)
  return constant * e_power

# Plot the normal distribution curve using plt.scatter()
plt.figure(figsize=(12,4),dpi=96)
plt.scatter(df['price'],prob_den_func(price))

plt.axvline(x=price.mean(),label=f'mean price of diamond={price.mean():.3f}',color='y')
plt.legend()

plt.show()

"""**Q:** What is the mean `price` of diamonds ?

**A:** The mean price of diamond is `3932.8`

---

#### 3. Feature Engineering

The dataset contains certain features that are categorical.  To convert these features into numerical ones, use `replace()` function of the DataFrame.

**For example:**

`df["column1"].replace({"a": 1, "b": 0}, inplace=True)` $\Rightarrow$ replaces all the `'a'` values with `1` and `'b'` values with `0` for feature `column1`. Use `inplace` boolean argument to to make changes in the DataFrame permanently.

Replace following values for `cut` column:

 - `Fair` with `1`
 - `Good` with `2`
 - `Very Good` with `3`
 - `Premium` with `4`
 - `Ideal` with `5`

Replace following values for the `color` column:

- `D` with `1`
- `E` with `2`
- `F` with `3`
- `G` with `4`
- `H` with `5`
- `I` with `6`

Replace following values for the `clarity` column:

- `I1` with `1`
- `SI2` with `2`
- `SI1` with `3`
- `VS2` with `4`
- `VS1` with `5`
- `VVS2` with `6`
- `VVS1` with `7`
- `IF` with `8`
"""

# Replace values of 'cut' column
df['cut'].replace({'Fair':1,'Good':2,'Very Good':3,'Premium':4,'Ideal':5},inplace=True)

# Replace values of 'color' column
df['color'].replace({'D':1,'E':2,'F':3,'G':4,'H':5,'I':6,'J':7},inplace=True)
df['color'] = pd.to_numeric(df['color'])

# Replace values of 'clarity' column
df['clarity'].replace({'I1':1,'SI2':2,'SI1':3,'VS2':4,'VS1':5,'VVS2':6,'VVS1':7,'IF':8},inplace=True)

df.head()
df.info()

"""---

#### 4. Model Training

Build a multiple linear regression model  using all the features of the dataset. Also, evaluate the model by calculating $R^2$, MSE, RMSE, and MAE values.
"""

# Create a list of feature variables.
features=list(df.columns.values)

features.remove('price')
features

# Build multiple linear regression model using all the features
from sklearn.model_selection import train_test_split


X=df[features]
y=df['price']
# Split the DataFrame into the train and test sets such that test set has 33% of the values.
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.33,random_state=42)

# Build linear regression model using the 'sklearn.linear_model' module.
from sklearn.linear_model import LinearRegression

y_train_reshaped=y_train.values.reshape(-1,1)
y_test_reshaped=y_test.values.reshape(-1,1)
lin_reg=LinearRegression()

lin_reg.fit(X_train,y_train_reshaped)
# Print the value of the intercept
print('Intercept -----',lin_reg.intercept_[0])

#Print the names of the features along with the values of their corresponding coefficients
for i in list(zip(X.columns.values,lin_reg.coef_[0])):
  print(f'{i[0]}'.ljust(15,'-'),f'{i[1]:.3f}')

# Evaluate the linear regression model using the 'r2_score', 'mean_squared_error' & 'mean_absolute_error' functions of the 'sklearn' module.
from sklearn.metrics import r2_score,mean_absolute_error,mean_squared_error
y_train_pred=lin_reg.predict(X_train)
y_test_pred=lin_reg.predict(X_test)
print(f"Train Set\n{'-' * 50}")
print(f"R-squared: {r2_score(y_train_reshaped, y_train_pred):.3f}")
print(f"Mean Squared Error: {mean_squared_error(y_train_reshaped, y_train_pred):.3f}")
print(f"Root Mean Squared Error: {np.sqrt(mean_squared_error(y_train_reshaped, y_train_pred)):.3f}")
print(f"Mean Absolute Error: {mean_absolute_error(y_train_reshaped, y_train_pred):.3f}")

print(f"\n\nTest Set\n{'-' * 50}")
print(f"R-squared: {r2_score(y_test_reshaped, y_test_pred):.3f}")
print(f"Mean Squared Error: {mean_squared_error(y_test_reshaped, y_test_pred):.3f}")
print(f"Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test_reshaped, y_test_pred)):.3f}")
print(f"Mean Absolute Error: {mean_absolute_error(y_test_reshaped, y_test_pred):.3f}")

"""**Q:** What is the $R^2$ (R-squared) value for this model?

**A:**  $R^2$ is 0.907 for this model

---

### 5. Dealing with Multicollinearity

Create a heatmap among all variables to identify a set of features which are highly correlated with each other.
"""

# Heatmap to pinpoint the columns in the 'df' DataFrame exhibiting high correlation
plt.figure(figsize=(12,4),dpi=96)
sns.heatmap(df.corr(),annot=True)
plt.show()

"""**Q:** Which features are highly correlated with `price`?

**A:** caret and dimensions(length(x),width(y),depth(z)) are highly correlated with price

**Q:** Is there multicollinearity in the dataset?

**A:** x,y,z are multicollinearity in the data set

Let's consider the feature `carat` as it is highly correlated with the target variable `price`. Perform the following tasks:
1. Drop the features which are highly correlated with `carat`.
2. Calculate VIF (Variance Inflation Factor) for the remaining features.
"""

# Drop features highly correlated with 'carat'
features.remove('carat')
features

# Again build a linear regression model using the remaining features
X=df[features]
y=df['carat']

# Build linear regression model using the 'sklearn.linear_model' module.
X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.33,random_state=42)


y_train_reshaped=y_train.values.reshape(-1,1)
y_test_reshaped=y_test.values.reshape(-1,1)

reg=LinearRegression()

reg.fit(X_train,y_train_reshaped)


# Print the value of the intercept
print(f'Intercept is {reg.intercept_[0]:.3f}')

# Print the names of the features along with the values of their corresponding coefficients.
for i in list(zip(X_train.columns.values,reg.coef_[0])):
  print(f'{i[0]}'.ljust(15,'_'),f'{i[1]:.3f}')

# Evaluate the linear regression model using the 'r2_score', 'mean_squared_error' & 'mean_absolute_error' functions of the 'sklearn' module.
caret_y_train_pred=reg.predict(X_train)
caret_y_test_pred=reg.predict(X_test)

print(f"Train Set\n{'-' * 50}")
print(f"R-squared: {r2_score(y_train_reshaped, caret_y_train_pred):.3f}")
print(f"Mean Squared Error: {mean_squared_error(y_train_reshaped, caret_y_train_pred):.3f}")
print(f"Root Mean Squared Error: {np.sqrt(mean_squared_error(y_train_reshaped, caret_y_train_pred)):.3f}")
print(f"Mean Absolute Error: {mean_absolute_error(y_train_reshaped, caret_y_train_pred):.3f}")

print(f"\n\nTest Set\n{'-' * 50}")
print(f"R-squared: {r2_score(y_test_reshaped, caret_y_test_pred):.3f}")
print(f"Mean Squared Error: {mean_squared_error(y_test_reshaped, caret_y_test_pred):.3f}")
print(f"Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test_reshaped, caret_y_test_pred)):.3f}")
print(f"Mean Absolute Error: {mean_absolute_error(y_test_reshaped, caret_y_test_pred):.3f}")

"""Now eliminate the features having VIF values above 10 (if any)."""

# Calculate the VIF values for the remaining features using the 'variance_inflation_factor' function.
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
# Add a constant to feature variables
X_train_sm=sm.add_constant(X_train)
# Create a dataframe that will contain the names of the feature variables and their respective VIFs
v_i_f=pd.DataFrame()
v_i_f['Features']=X_train_sm.columns
v_i_f['VIF']=[variance_inflation_factor(X_train_sm.values,i) for i in range(X_train_sm.values.shape[1])]
v_i_f['VIF']=round(v_i_f['VIF'],2)
v_i_f

"""**Q**: Which of the features have VIF values above 10?

**A**: x,y,z and constant have VIF values above 10

Proceed with the below 4 code cells only if any of the features have VIF value above 10, otherwise jump to **6. Residual (Error) Analysis** section.
"""

# Create a list of features having VIF values less than 10
vif_list=['cut','color','clarity','depth','table']
remain_vif_list=['const','x','y','z']
r_X_train=X_train_sm[remain_vif_list]

# Again build a linear regression model using the features whose VIF values are less than 10
f_X_train=X_train[vif_list]
f_X_test=X_test[vif_list]


# Build linear regression model using the 'sklearn.linear_model' module.

vif_reg_lin=LinearRegression()
vif_reg_lin.fit(f_X_train,y_train_reshaped)
# Print the value of the intercept
print(f'Intercept = {vif_reg_lin.intercept_[0]:.3f}')

# Print the names of the features along with the values of their corresponding coefficients.
for i in list(zip(f_X_train.columns.values,vif_reg_lin.coef_[0])):
  print(f'{i[0]}'.ljust(15,'-'),f'{i[1]:.3f}')

# Evaluate the linear regression model using the 'r2_score', 'mean_squared_error' & 'mean_absolute_error' functions of the 'sklearn' module.
f_y_train_pred=vif_reg_lin.predict(f_X_train)
f_y_test_pred=vif_reg_lin.predict(f_X_test)

print(f"Train Set\n{'-' * 50}")
print(f"R-squared: {r2_score(y_train_reshaped, f_y_train_pred):.3f}")
print(f"Mean Squared Error: {mean_squared_error(y_train_reshaped, f_y_train_pred):.3f}")
print(f"Root Mean Squared Error: {np.sqrt(mean_squared_error(y_train_reshaped, f_y_train_pred)):.3f}")
print(f"Mean Absolute Error: {mean_absolute_error(y_train_reshaped, f_y_train_pred):.3f}")

print(f"\n\nTest Set\n{'-' * 50}")
print(f"R-squared: {r2_score(y_test_reshaped, f_y_test_pred):.3f}")
print(f"Mean Squared Error: {mean_squared_error(y_test_reshaped, f_y_test_pred):.3f}")
print(f"Root Mean Squared Error: {np.sqrt(mean_squared_error(y_test_reshaped, f_y_test_pred)):.3f}")
print(f"Mean Absolute Error: {mean_absolute_error(y_test_reshaped, f_y_test_pred):.3f}")

# Again calculate the VIF values for the remaining features to find out if there is still multicollinearit
r_X_train_sm=sm.add_constant(r_X_train)
# Create a dataframe that will contain the names of the feature variables and their respective VIFs
vif=pd.DataFrame()
vif['Features']=r_X_train_sm.columns
vif['VIF']=[variance_inflation_factor(r_X_train_sm.values,i) for i in range(r_X_train_sm.values.shape[1])]
vif['VIF']=round(vif['VIF'],2)
vif

"""---

#### 6. Residual (Error) Analysis

Perform residual analysis to check if the residuals (errors) are normally distributed or not (which is one of the assumption of linear regression). For this, plot the  histogram of the residuals.
"""

# Create a histogram for the errors obtained in the predicted values for the train set.
error_train_set=y_train_reshaped-y_train_pred

plt.figure(figsize=(12,4),dpi=96)
plt.title(' histogram for the errors obtained in the predicted values for the train set')
plt.hist(error_train_set,bins='sturges')
plt.axvline(x=error_train_set.mean(),label=f'train mean error {error_train_set.mean():.3f}',color='r')
plt.legend()
plt.show()

# Create a histogram for the errors obtained in the predicted values for the test set.
error_test_set=y_test_reshaped-y_test_pred

plt.figure(figsize=(12,4),dpi=96)
plt.title(' histogram for the errors obtained in the predicted values for the train set')
plt.hist(error_test_set,bins='sturges')
plt.axvline(x=error_test_set.mean(),label=f'train mean error {error_test_set.mean():.3f}',color='r')
plt.legend()
plt.show()

"""**Q:** Is the mean of errors equal to 0 for train set?

**A:** mean of errors is -3951.756

**Q:** Is the mean of errors equal to 0 for test set?

**A:** mean of error is -3902.386

---

#### 7. Verify Homoscedasticity

Check for Homoscedasticity (constant variance) by creating a scatter plot between the errors and the target variable. Determine whether there is some kind of relationship between the error and the target variable.
"""

# Create a scatter plot between the errors and the dependent variable for the train set.
plt.figure(figsize=(12,4))
plt.scatter(y_train_reshaped,error_train_set)
plt.axhline(y=error_test_set.mean(),color='r',label=f'mean of errors= {error_test_set.mean():.3f}')
plt.legend()
plt.show()



"""https://www.hindawi.com/journals/jhe/2022/2826127/

After running the code above, you should see a public URL printed in the output. Click on that URL to access your Flask application's frontend.

Remember to add your `ngrok` authtoken to Colab's secrets manager for secure access.
"""


import joblib

# Save the model and column info
joblib.dump(lin_reg, 'model.pkl')
joblib.dump(features, 'model_features.pkl')
print("Model saved successfully!")


